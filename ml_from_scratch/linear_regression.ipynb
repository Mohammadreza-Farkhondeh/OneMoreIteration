{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "\tdef __init__(self, learning_rate=0.001, num_iters=1000):\n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.num_iters = num_iters\n",
    "\t\tself.weights = None\n",
    "\t\tself.bias = None\n",
    "\tdef fit(self, x, y):\n",
    "\t\tbetas = self._estimate_coefficients(x, y)\n",
    "\n",
    "\t\tself.intercept = betas[0]\n",
    "\n",
    "\t\tself.coefficients = betas[1:]\n",
    "\n",
    "\t# def fit(self, X, y):\n",
    "\n",
    "\n",
    "\t\t# '''Because X will be dotted in weights, and weights looks like [b0, b1, b2, ...].\n",
    "\t\t# This means that b1 should be multiplied in x1, but what is b0, its the bias.\n",
    "\t\t# So we need to multiply it in 1 (b2x2 + b1x1, b0x0), since we have no x0,\n",
    "\t\t# it should be filled with ones.'''\n",
    "\t\t# X_with_bias = np.c_[np.ones(X.shape[0]), X]\n",
    "\t\t# self.weights = np.zeros(X_with_bias.shape[1])\n",
    "\n",
    "\t\t# for _ in range(self.num_iters):\n",
    "\t\t# \tpredicted_y = np.dot(X_with_bias, self.weights)\n",
    "\n",
    "\t\t# \terrors = predicted_y - y\n",
    "\n",
    "\t\t# \t'''Update weights and bias using gradient descent with a detailed explanation\n",
    "\t\t# \tGradient descent aims to minimize the cost function (often the mean squared error).\n",
    "\t\t# \tWe calculate the gradients of the cost function with respect to the weights and bias.\n",
    "\t\t# \tThe gradients represent the direction and magnitude of change required to adjust the parameters\n",
    "\t\t# \tand reduce the cost function in the next iteration.\n",
    "\n",
    "\t\t# \tGradient of cost function w.r.t. weights:\n",
    "\t\t# \t(1/n) * 2 * X_with_bias.T * errors  # Multiplication by 2 for clarity in gradient update\n",
    "\t\t# \tThe factor of 2 comes from the derivative of the mean squared error cost function.\n",
    "\t\t# \tWe can simplify it to: (1/n) * X_with_bias.T * errors\n",
    "\n",
    "\t\t# \tGradient of cost function w.r.t. bias:\n",
    "\t\t# \t(1/n) * 2 * np.sum(errors)  # Multiplication by 2 for clarity'''\n",
    "\t\t# \tprint(self.weights)\n",
    "\t\t# \tprint(X_with_bias)\n",
    "\t\t# \tprint(errors)\n",
    "\t\t# \tprint(\"_+O_\")\n",
    "\t\t# \tself.weights -= self.learning_rate * np.dot(X_with_bias.T, errors)\n",
    "\t\t# \tself.bias = self.weights[0]\n",
    "\n",
    "\tdef predict(self, x):\n",
    "\t\t'''\n",
    "\t\t\ty = b_0 + b_1*x + ... + b_i*x_i\n",
    "\t\t'''\n",
    "\t\tpredictions = []\n",
    "\t\tfor row in x:\n",
    "\t\t\tpred = np.multiply(row, self.coefficients)\n",
    "\t\t\tpred = sum(pred)\n",
    "\t\t\tpred += self.intercept\n",
    "\n",
    "\t\t\tpredictions.append(pred)\n",
    "\n",
    "\t\treturn predictions\n",
    "\n",
    "\tdef r2_score(self, y_true, y_pred):\n",
    "\t\t'''\n",
    "\t\t\tr2 = 1 - (rss/tss)\n",
    "\t\t\trss = sum_{i=0}^{n} (y_i - y_hat)^2\n",
    "\t\t\ttss = sum_{i=0}^{n} (y_i - y_bar)^2\n",
    "\t\t'''\n",
    "\t\ty_average = np.average(y_true)\n",
    "\n",
    "\t\tresidual_sum_of_squares = 0\n",
    "\t\ttotal_sum_of_squares = 0\n",
    "\n",
    "\t\tfor i in range(len(y_true)):\n",
    "\t\t\tresidual_sum_of_squares += (y_true[i] - y_pred[i])**2\n",
    "\t\t\ttotal_sum_of_squares += (y_true[i] - y_average)**2\n",
    "\n",
    "\t\treturn 1 - (residual_sum_of_squares/total_sum_of_squares)\n",
    "\n",
    "\tdef _estimate_coefficients(self, x, y):\n",
    "\t\t'''\n",
    "\t\t\tÎ² = (X^T X)^-1 X^T y\n",
    "\t\t\tEstimates both the intercept and all coefficients.\n",
    "\t\t'''\n",
    "\t\txT = x.transpose()\n",
    "\n",
    "\t\tinversed = np.linalg.inv( xT.dot(x) )\n",
    "\t\tcoefficients = inversed.dot( xT ).dot(y)\n",
    "\n",
    "\t\treturn coefficients\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for new data point (x = 5): 1.6333333333333333\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 4, 5, 6])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "new_data = np.array([[5]])\n",
    "prediction = model.predict(new_data)\n",
    "\n",
    "print(f\"Prediction for new data point (x = 5): {prediction[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18457.802597727466\n",
      "-1.2725704452575575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('insurance.csv')\n",
    "y = df[\"payments\"].to_numpy()\n",
    "X = df.drop('payments', axis=1).to_numpy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(mae)\n",
    "\n",
    "\n",
    "r2 = model.r2_score(y_test, y_pred)\n",
    "print(r2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
